{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: feature number annotation can be found in pos.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the each dataset and store to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/K1_dataset.txt\") as f:\n",
    "    K1_txt = f.read()\n",
    "\n",
    "with open(\"./data/K2_dataset.txt\") as f:\n",
    "    K2_txt = f.read()\n",
    "\n",
    "with open(\"./data/Q_dataset.txt\") as f:\n",
    "    Q_txt = f.read()\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    [\"K1\", K1_txt], \n",
    "    [\"K2\", K2_txt],\n",
    "    [\"Q\", Q_txt]\n",
    "],\n",
    "columns=['author', 'message'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K1</td>\n",
       "      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K2</td>\n",
       "      <td>\\n\\nWith the rapid growth of the information c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q</td>\n",
       "      <td>\\n\\nHowever, there are frequent situations whe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author                                            message\n",
       "0     K1  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...\n",
       "1     K2  \\n\\nWith the rapid growth of the information c...\n",
       "2      Q  \\n\\nHowever, there are frequent situations whe..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of tuples of (word, POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function returns a list of tuples of (word, PoS) from a document.\n",
    "'''\n",
    "def create_word_pos_list(message):\n",
    "    tokenized_txt = nltk.word_tokenize(message)\n",
    "    return nltk.pos_tag(tokenized_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(2,\"word_pos_list\",[ create_word_pos_list(message) for message in df['message']],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('However', 'RB'), (',', ','), ('there', 'EX')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_pos_list'][2][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mサンプル文字列\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RED = '\\033[31m'\n",
    "GREEN = '\\033[32m'\n",
    "END = '\\033[0m'\n",
    "print(GREEN + \"サンプル文字列\" + END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"In\".casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_string):\n",
    "    query = query_string.split()\n",
    "    print(\"query : \", query)\n",
    "    for docId, document in enumerate(df['word_pos_list']):\n",
    "        for idx, (word, pos), in enumerate(document):\n",
    "            # print(f'(word, pos): {word}, {pos}')\n",
    "            if word.casefold() == query[0].casefold(): # undistinguith Upper or Lower case\n",
    "                # check the following words matche the sequence of words\n",
    "                foundFlg = True\n",
    "                # print(\"Following \", query[1:])\n",
    "\n",
    "                if len(query)>1:\n",
    "\n",
    "                    for i, query_following in enumerate(query[1:]):\n",
    "                        try:\n",
    "                            word_following = document[idx+1+i][0] # follwoing word in the document\n",
    "                            pos_following = document[idx+1+i][1] # follwoing pos in the document\n",
    "                        except: # if cannot access the index\n",
    "                            break\n",
    "                        if query_following.casefold() == word_following.casefold() or query_following == pos_following:\n",
    "                            continue\n",
    "                        foundFlg = False\n",
    "                    \n",
    "                if foundFlg is True:\n",
    "                    if docId == 0:\n",
    "                        print(\"K1:  \", end=\"\")\n",
    "                    elif docId == 1:\n",
    "                        print(\"K2:  \", end=\"\")\n",
    "                    elif docId == 2:\n",
    "                        print(\"Q :  \", end=\"\")\n",
    "\n",
    "\n",
    "                    # # 前後6 文字　出力\n",
    "                    for j in range(idx-6, idx+7):\n",
    "                            if 0 <= j and  j < len(document):\n",
    "                                if idx <= j and j < idx + len(query):  # within queried words\n",
    "                                    print(GREEN + document[j][0] + END, end=\" \")\n",
    "                                else:    \n",
    "                                    print(document[j][0], end=\" \")\n",
    "                            else:\n",
    "                                print(\"    \", end=\"\")\n",
    "\n",
    "                    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query :  ['appropriate', 'conferences']\n",
      "Q :  authors could consider a range of \u001b[32mappropriate\u001b[0m \u001b[32mconferences\u001b[0m , while the exact venue \n",
      "Q :  authors could consider a range of \u001b[32mappropriate\u001b[0m \u001b[32mconferences\u001b[0m , while the exact venue \n"
     ]
    }
   ],
   "source": [
    "search(\"appropriate conferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query :  ['how', 'JJ', 'NNS']\n",
      "K1:  . Our idea is to investigate \u001b[32mhow\u001b[0m \u001b[32mcomputational\u001b[0m \u001b[32mmodels\u001b[0m can enhance musicology research \n"
     ]
    }
   ],
   "source": [
    "search(\"how JJ NNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query :  ['services', 'NN']\n",
      "Q :  { hasan2013understanding } . Such proactive \u001b[32mservices\u001b[0m \u001b[32mhelp\u001b[0m to overcome the limitations of \n",
      "Q :  23 ] . Such proac- tive \u001b[32mservices\u001b[0m \u001b[32mhelp\u001b[0m to overcome the limitations of \n"
     ]
    }
   ],
   "source": [
    "search(\"services NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## suspect の前処理\n",
    " - 全てのデータせっとに現れるPOSパターンのリストを作成\n",
    " - 同時にそれぞれのデータセットでカウントしていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "def count_pos_patterns(documents):\n",
    "    width = WINDOW_SIZE\n",
    "    pos_patterns = []\n",
    "    pos_vectors = [[] for _ in range(len(documents))]\n",
    "    for docId, document in enumerate(documents):\n",
    "        len_doc = len(document)\n",
    "        for i in range(len_doc - (width-1)):\n",
    "            key_str = \"\"\n",
    "            key_str += document[i][0] # word\n",
    "\n",
    "            for j in range(1, width):\n",
    "                key_str += \" \" + document[i+j][1] # POS\n",
    "\n",
    "            if key_str not in pos_patterns:\n",
    "                pos_patterns.append(key_str)\n",
    "                # パターンが現れたドキュメント -> 1 , それ以外　-> 0\n",
    "                for i in range(len(pos_vectors)): # loop for K1, K2, Q\n",
    "                    if i == docId:\n",
    "                        pos_vectors[i].append(1)\n",
    "                    else:\n",
    "                        pos_vectors[i].append(0)\n",
    "\n",
    "            else: \n",
    "                idx = pos_patterns.index(key_str)\n",
    "                pos_vectors[docId][idx] += 1\n",
    "\n",
    "    return (pos_patterns, pos_vectors)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>message</th>\n",
       "      <th>word_pos_list</th>\n",
       "      <th>pos_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K1</td>\n",
       "      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n",
       "      <td>[(Download, NNP), (Source, NNP), (PDF, NNP), (...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K2</td>\n",
       "      <td>\\n\\nWith the rapid growth of the information c...</td>\n",
       "      <td>[(With, IN), (the, DT), (rapid, JJ), (growth, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q</td>\n",
       "      <td>\\n\\nHowever, there are frequent situations whe...</td>\n",
       "      <td>[(However, RB), (,, ,), (there, EX), (are, VBP...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author                                            message  \\\n",
       "0     K1  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n",
       "1     K2  \\n\\nWith the rapid growth of the information c...   \n",
       "2      Q  \\n\\nHowever, there are frequent situations whe...   \n",
       "\n",
       "                                       word_pos_list  \\\n",
       "0  [(Download, NNP), (Source, NNP), (PDF, NNP), (...   \n",
       "1  [(With, IN), (the, DT), (rapid, JJ), (growth, ...   \n",
       "2  [(However, RB), (,, ,), (there, EX), (are, VBP...   \n",
       "\n",
       "                                             pos_vec  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_patterns, pos_vectors = count_pos_patterns(df['word_pos_list'])\n",
    "df['pos_vec'] = pos_vectors\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the NN'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "max_idx = pos_vectors[i].index(max(pos_vectors[i]))\n",
    "pos_patterns[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start からend までのIDの配列を返す\n",
    "def extract_features(freq_vector, start=0, end=20):\n",
    "\n",
    "    setX = set(freq_vector) # 最大値を順に取り出すため set を作成\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    result = []\n",
    "\n",
    "    while count<end:\n",
    "        try:\n",
    "            max_value = max(setX)\n",
    "        except ValueError:\n",
    "            return result\n",
    "\n",
    "        max_index = freq_vector.index(max_value)\n",
    "        # max_word = words[max_index]\n",
    "\n",
    "        setX.remove(max_value)\n",
    "\n",
    "        if count>= start:\n",
    "            result.append(max_index)\n",
    "        count += 1\n",
    "\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the score of how many types of pattern are appered in both vectors?\n",
    "def get_similarity(feature_vector1,feature_vector2): \n",
    "    return len(set(feature_vector1) & set(feature_vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1_features = extract_features(pos_vectors[0], 0, 60)\n",
    "K2_features = extract_features(pos_vectors[1], 0, 60)\n",
    "Q_features = extract_features(pos_vectors[2], 0, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity(K1_features, Q_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Q_df,K_df):\n",
    "    start = 0\n",
    "    end = 20\n",
    "    suspected = [author for author in K_df['author'] ]\n",
    "    while(len(suspected) > 1):\n",
    "        print(\"Suspected : \", end=\"\")\n",
    "        print(set(suspected))\n",
    "        Q_features = extract_features(Q_df['pos_vec'], start, end)\n",
    "        similarityWithQ = {}\n",
    "\n",
    "        for author, reference_vector in (K_df['author'], K_df['pos_vec']):\n",
    "            if author in suspected: #\n",
    "                feature_vector = extract_features(reference_vector,start, end)\n",
    "                score = get_similarity(feature_vector,Q_features)\n",
    "                similarityWithQ[author]=score\n",
    "\n",
    "        # innocent_list に含まれない著者の中から1人を選ぶ\n",
    "\n",
    "        innocent = min(similarityWithQ, key=similarityWithQ.get)\n",
    "        if input(f'Do you want to rule out {innocent} in top {end} patterns ? (y/n) ') == 'y':\n",
    "            suspected.remove(innocent)\n",
    "        end += 20\n",
    "    return suspected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspected : {'K2', 'K1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'K2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict(df[:][1])\n",
    "K_df = df.loc[0:1]\n",
    "Q_df = df.loc[2]\n",
    "predict(Q_df, K_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expsystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
